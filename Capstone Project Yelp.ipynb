{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T19:17:07.054179Z",
     "start_time": "2021-12-14T19:17:06.990609Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "from IPython.display import display, HTML\n",
    "data_path=\"./data/yelp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "In this project we will be building analytics database using [Yelp dataset](https://www.yelp.com/dataset). This analytics table can be used to answer questions like: \n",
    "- How different discount, customer relationship programs, etc on reviews. So basically data analyst should be able to run a query and see how the reviews score changes during program time vs other times\n",
    "- Have ability to fitler out negative(less than average review score for given business)reviews and find top used words/phrases\n",
    "- Find top users that provided most value to the business using reviews/tips\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "Dataset contains a number of of newline delimeted json files.\n",
    "\n",
    "#### Dataset description \n",
    "From [the dataset description](https://www.yelp.com/dataset/documentation/main)\n",
    "\n",
    "Each file is composed of a single object type, one JSON-object per-line.\n",
    "\n",
    "Take a look at some examples to get you started: https://github.com/Yelp/dataset-examples.\n",
    "\n",
    "Note: the follow examples contain inline comments, which are technically not valid JSON. This is done here to simplify the documentation and explaining the structure, the JSON files you download will not contain any comments and will be fully valid JSON.\n",
    "\n",
    "Sources:\n",
    "\n",
    "- business.json - Contains business data including location data, attributes, and categories.\n",
    "\n",
    "```json \n",
    "{\n",
    "    // string, 22 character unique string business id\n",
    "    \"business_id\": \"tnhfDv5Il8EaGSXZGiuQGg\",\n",
    "\n",
    "    // string, the business's name\n",
    "    \"name\": \"Garaje\",\n",
    "\n",
    "    // string, the full address of the business\n",
    "    \"address\": \"475 3rd St\",\n",
    "\n",
    "    // string, the city\n",
    "    \"city\": \"San Francisco\",\n",
    "\n",
    "    // string, 2 character state code, if applicable\n",
    "    \"state\": \"CA\",\n",
    "\n",
    "    // string, the postal code\n",
    "    \"postal code\": \"94107\",\n",
    "\n",
    "    // float, latitude\n",
    "    \"latitude\": 37.7817529521,\n",
    "\n",
    "    // float, longitude\n",
    "    \"longitude\": -122.39612197,\n",
    "\n",
    "    // float, star rating, rounded to half-stars\n",
    "    \"stars\": 4.5,\n",
    "\n",
    "    // integer, number of reviews\n",
    "    \"review_count\": 1198,\n",
    "\n",
    "    //TODO: convert to boolean\n",
    "\n",
    "    // integer, 0 or 1 for closed or open, respectively\n",
    "    \"is_open\": 1,\n",
    "\n",
    "    //TODO: do we need this?\n",
    "    \n",
    "    // object, business attributes to values. note: some attribute values might be objects\n",
    "    \"attributes\": {\n",
    "        \"RestaurantsTakeOut\": true,\n",
    "        \"BusinessParking\": {\n",
    "            \"garage\": false,\n",
    "            \"street\": true,\n",
    "            \"validated\": false,\n",
    "            \"lot\": false,\n",
    "            \"valet\": false\n",
    "        },\n",
    "    },\n",
    "\n",
    "    // an array of strings of business categories\n",
    "    \"categories\": [\n",
    "        \"Mexican\",\n",
    "        \"Burgers\",\n",
    "        \"Gastropubs\"\n",
    "    ],\n",
    "\n",
    "    // an object of key day to value hours, hours are using a 24hr clock\n",
    "    \"hours\": {\n",
    "        \"Monday\": \"10:00-21:00\",\n",
    "        \"Tuesday\": \"10:00-21:00\",\n",
    "        \"Friday\": \"10:00-21:00\",\n",
    "        \"Wednesday\": \"10:00-21:00\",\n",
    "        \"Thursday\": \"10:00-21:00\",\n",
    "        \"Sunday\": \"11:00-18:00\",\n",
    "        \"Saturday\": \"10:00-21:00\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "- review.json - Contains full review text data including the user_id that wrote the review and the business_id the review is written for.\n",
    "```json\n",
    "{\n",
    "    // string, 22 character unique review id\n",
    "    \"review_id\": \"zdSx_SD6obEhz9VrW9uAWA\",\n",
    "\n",
    "    // string, 22 character unique user id, maps to the user in user.json\n",
    "    \"user_id\": \"Ha3iJu77CxlrFm-vQRs_8g\",\n",
    "\n",
    "    // string, 22 character business id, maps to business in business.json\n",
    "    \"business_id\": \"tnhfDv5Il8EaGSXZGiuQGg\",\n",
    "\n",
    "    // integer, star rating\n",
    "    \"stars\": 4,\n",
    "\n",
    "    // string, date formatted YYYY-MM-DD\n",
    "    \"date\": \"2016-03-09\",\n",
    "\n",
    "    // string, the review itself\n",
    "    \"text\": \"Great place to hang out after work: the prices are decent, and the ambience is fun. It's a bit loud, but very lively. The staff is friendly, and the food is good. They have a good selection of drinks.\",\n",
    "\n",
    "    //TODO: check min max for next values\n",
    "    \n",
    "    // integer, number of useful votes received\n",
    "    \"useful\": 0,\n",
    "\n",
    "    // integer, number of funny votes received\n",
    "    \"funny\": 0,\n",
    "\n",
    "    // integer, number of cool votes received\n",
    "    \"cool\": 0\n",
    "}\n",
    "```\n",
    "- user.json - User data including the user's friend mapping and all the metadata associated with the user.\n",
    "```json\n",
    "{\n",
    "    // string, 22 character unique user id, maps to the user in user.json\n",
    "    \"user_id\": \"Ha3iJu77CxlrFm-vQRs_8g\",\n",
    "\n",
    "    // string, the user's first name\n",
    "    \"name\": \"Sebastien\",\n",
    "\n",
    "    // integer, the number of reviews they've written\n",
    "    \"review_count\": 56,\n",
    "\n",
    "    // string, when the user joined Yelp, formatted like YYYY-MM-DD\n",
    "    \"yelping_since\": \"2011-01-01\",\n",
    "\n",
    "    // array of strings, an array of the user's friend as user_ids\n",
    "    \"friends\": [\n",
    "        \"wqoXYLWmpkEH0YvTmHBsJQ\",\n",
    "        \"KUXLLiJGrjtSsapmxmpvTA\",\n",
    "        \"6e9rJKQC3n0RSKyHLViL-Q\"\n",
    "    ],\n",
    "\n",
    "    // integer, number of useful votes sent by the user\n",
    "    \"useful\": 21,\n",
    "\n",
    "    // integer, number of funny votes sent by the user\n",
    "    \"funny\": 88,\n",
    "\n",
    "    // integer, number of cool votes sent by the user\n",
    "    \"cool\": 15,\n",
    "\n",
    "    // integer, number of fans the user has\n",
    "    \"fans\": 1032,\n",
    "\n",
    "    // array of integers, the years the user was elite\n",
    "    \"elite\": [\n",
    "        2012,\n",
    "        2013\n",
    "    ],\n",
    "\n",
    "    //TODO: do we need this?\n",
    "    \n",
    "    // float, average rating of all reviews\n",
    "    \"average_stars\": 4.31,\n",
    "\n",
    "    // integer, number of hot compliments received by the user\n",
    "    \"compliment_hot\": 339,\n",
    "\n",
    "    // integer, number of more compliments received by the user\n",
    "    \"compliment_more\": 668,\n",
    "\n",
    "    // integer, number of profile compliments received by the user\n",
    "    \"compliment_profile\": 42,\n",
    "\n",
    "    // integer, number of cute compliments received by the user\n",
    "    \"compliment_cute\": 62,\n",
    "\n",
    "    // integer, number of list compliments received by the user\n",
    "    \"compliment_list\": 37,\n",
    "\n",
    "    // integer, number of note compliments received by the user\n",
    "    \"compliment_note\": 356,\n",
    "\n",
    "    // integer, number of plain compliments received by the user\n",
    "    \"compliment_plain\": 68,\n",
    "\n",
    "    // integer, number of cool compliments received by the user\n",
    "    \"compliment_cool\": 91,\n",
    "\n",
    "    // integer, number of funny compliments received by the user\n",
    "    \"compliment_funny\": 99,\n",
    "\n",
    "    // integer, number of writer compliments received by the user\n",
    "    \"compliment_writer\": 95,\n",
    "\n",
    "    // integer, number of photo compliments received by the user\n",
    "    \"compliment_photos\": 50\n",
    "}\n",
    "```\n",
    "\n",
    "- checkin.json - Checkins on a business.\n",
    "```json\n",
    "{\n",
    "    // string, 22 character business id, maps to business in business.json\n",
    "    \"business_id\": \"tnhfDv5Il8EaGSXZGiuQGg\"\n",
    "\n",
    "    // string which is a comma-separated list of timestamps for each checkin, each with format YYYY-MM-DD HH:MM:SS\n",
    "    \"date\": \"2016-04-26 19:49:16, 2016-08-30 18:36:57, 2016-10-15 02:45:18, 2016-11-18 01:54:50, 2017-04-20 18:39:06, 2017-05-03 17:58:02\"\n",
    "}\n",
    "```\n",
    "- tip.json - Tips written by a user on a business. Tips are shorter than reviews and tend to convey quick suggestions.\n",
    "```json\n",
    "{\n",
    "    // string, text of the tip\n",
    "    \"text\": \"Secret menu - fried chicken sando is da bombbbbbb Their zapatos are good too.\",\n",
    "\n",
    "    // string, when the tip was written, formatted like YYYY-MM-DD\n",
    "    \"date\": \"2013-09-20\",\n",
    "\n",
    "    // integer, how many compliments it has\n",
    "    \"compliment_count\": 172,\n",
    "\n",
    "    // string, 22 character business id, maps to business in business.json\n",
    "    \"business_id\": \"tnhfDv5Il8EaGSXZGiuQGg\",\n",
    "\n",
    "    // string, 22 character unique user id, maps to the user in user.json\n",
    "    \"user_id\": \"49JhAJh8vSQ-vM4Aourl0g\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:41:56.977098Z",
     "start_time": "2021-12-05T18:41:56.975013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:42:01.856859Z",
     "start_time": "2021-12-05T18:41:56.978263Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.53 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/05 20:41:58 WARN Utils: Your hostname, babkamen-Lenovo resolves to a loopback address: 127.0.1.1; using 192.168.0.133 instead (on interface wlp3s0)\n",
      "21/12/05 20:41:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/babkamen/git/udacity-big-data-engineer-nanodegree/capstone-project/venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/05 20:41:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "%time\n",
    "spark = SparkSession.builder.master(\"local[8]\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "prev = spark.conf.get(\"spark.sql.execution.arrow.enabled\")  # Keep its default value.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings coming from Arrow optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:42:01.871433Z",
     "start_time": "2021-12-05T18:42:01.861379Z"
    }
   },
   "outputs": [],
   "source": [
    "def info(df):\n",
    "    display(df.info())\n",
    "    \n",
    "    print(\"Null values count:\")\n",
    "    print()\n",
    "    c=df.isnull().sum()\n",
    "    print(c[c>0])\n",
    "#     TODO: add duplicated values count\n",
    "    print()\n",
    "    print(\"Statistics:\")\n",
    "    display(df.describe(include='all'))\n",
    "    \n",
    "    print(\"Head:\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:42:17.515344Z",
     "start_time": "2021-12-05T18:42:01.873263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/05 20:42:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160585 entries, 0 to 160584\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   address       160585 non-null  object \n",
      " 1   attributes    145593 non-null  object \n",
      " 2   business_id   160585 non-null  object \n",
      " 3   categories    160470 non-null  object \n",
      " 4   city          160585 non-null  object \n",
      " 5   hours         133244 non-null  object \n",
      " 6   is_open       160585 non-null  int64  \n",
      " 7   latitude      160585 non-null  float64\n",
      " 8   longitude     160585 non-null  float64\n",
      " 9   name          160585 non-null  object \n",
      " 10  postal_code   160585 non-null  object \n",
      " 11  review_count  160585 non-null  int64  \n",
      " 12  stars         160585 non-null  float64\n",
      " 13  state         160585 non-null  object \n",
      "dtypes: float64(3), int64(2), object(9)\n",
      "memory usage: 17.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values count:\n",
      "\n",
      "attributes    14992\n",
      "categories      115\n",
      "hours         27341\n",
      "dtype: int64\n",
      "\n",
      "Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>160585</td>\n",
       "      <td>145593</td>\n",
       "      <td>160585</td>\n",
       "      <td>160470</td>\n",
       "      <td>160585</td>\n",
       "      <td>133244</td>\n",
       "      <td>160585.000000</td>\n",
       "      <td>160585.000000</td>\n",
       "      <td>160585.000000</td>\n",
       "      <td>160585</td>\n",
       "      <td>160585</td>\n",
       "      <td>160585.000000</td>\n",
       "      <td>160585.000000</td>\n",
       "      <td>160585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>123895</td>\n",
       "      <td>67907</td>\n",
       "      <td>160585</td>\n",
       "      <td>88115</td>\n",
       "      <td>836</td>\n",
       "      <td>50857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125850</td>\n",
       "      <td>5779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "      <td>(None, None, None, None, None, None, None, Non...</td>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>Beauty &amp; Spas, Hair Salons</td>\n",
       "      <td>Austin</td>\n",
       "      <td>(0:0-0:0, 0:0-0:0, 0:0-0:0, 0:0-0:0, 0:0-0:0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>78704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6726</td>\n",
       "      <td>9316</td>\n",
       "      <td>1</td>\n",
       "      <td>757</td>\n",
       "      <td>22416</td>\n",
       "      <td>5708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>852</td>\n",
       "      <td>2084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.767494</td>\n",
       "      <td>38.759794</td>\n",
       "      <td>-94.266212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.964548</td>\n",
       "      <td>3.656954</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.422431</td>\n",
       "      <td>7.138042</td>\n",
       "      <td>19.975446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.030448</td>\n",
       "      <td>0.943604</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.998972</td>\n",
       "      <td>-123.393929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.355886</td>\n",
       "      <td>-122.589583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.177366</td>\n",
       "      <td>-84.383281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.458531</td>\n",
       "      <td>-81.288501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.490000</td>\n",
       "      <td>71.113271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9185.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       address                                         attributes  \\\n",
       "count   160585                                             145593   \n",
       "unique  123895                                              67907   \n",
       "top             (None, None, None, None, None, None, None, Non...   \n",
       "freq      6726                                               9316   \n",
       "mean       NaN                                                NaN   \n",
       "std        NaN                                                NaN   \n",
       "min        NaN                                                NaN   \n",
       "25%        NaN                                                NaN   \n",
       "50%        NaN                                                NaN   \n",
       "75%        NaN                                                NaN   \n",
       "max        NaN                                                NaN   \n",
       "\n",
       "                   business_id                  categories    city  \\\n",
       "count                   160585                      160470  160585   \n",
       "unique                  160585                       88115     836   \n",
       "top     6iYb2HFDywm3zjuRg0shjw  Beauty & Spas, Hair Salons  Austin   \n",
       "freq                         1                         757   22416   \n",
       "mean                       NaN                         NaN     NaN   \n",
       "std                        NaN                         NaN     NaN   \n",
       "min                        NaN                         NaN     NaN   \n",
       "25%                        NaN                         NaN     NaN   \n",
       "50%                        NaN                         NaN     NaN   \n",
       "75%                        NaN                         NaN     NaN   \n",
       "max                        NaN                         NaN     NaN   \n",
       "\n",
       "                                                    hours        is_open  \\\n",
       "count                                              133244  160585.000000   \n",
       "unique                                              50857            NaN   \n",
       "top     (0:0-0:0, 0:0-0:0, 0:0-0:0, 0:0-0:0, 0:0-0:0, ...            NaN   \n",
       "freq                                                 5708            NaN   \n",
       "mean                                                  NaN       0.767494   \n",
       "std                                                   NaN       0.422431   \n",
       "min                                                   NaN       0.000000   \n",
       "25%                                                   NaN       1.000000   \n",
       "50%                                                   NaN       1.000000   \n",
       "75%                                                   NaN       1.000000   \n",
       "max                                                   NaN       1.000000   \n",
       "\n",
       "             latitude      longitude       name postal_code   review_count  \\\n",
       "count   160585.000000  160585.000000     160585      160585  160585.000000   \n",
       "unique            NaN            NaN     125850        5779            NaN   \n",
       "top               NaN            NaN  Starbucks       78704            NaN   \n",
       "freq              NaN            NaN        852        2084            NaN   \n",
       "mean        38.759794     -94.266212        NaN         NaN      51.964548   \n",
       "std          7.138042      19.975446        NaN         NaN     130.030448   \n",
       "min         27.998972    -123.393929        NaN         NaN       5.000000   \n",
       "25%         30.355886    -122.589583        NaN         NaN       8.000000   \n",
       "50%         42.177366     -84.383281        NaN         NaN      17.000000   \n",
       "75%         45.458531     -81.288501        NaN         NaN      44.000000   \n",
       "max         49.490000      71.113271        NaN         NaN    9185.000000   \n",
       "\n",
       "                stars   state  \n",
       "count   160585.000000  160585  \n",
       "unique            NaN      31  \n",
       "top               NaN      MA  \n",
       "freq              NaN   36012  \n",
       "mean         3.656954     NaN  \n",
       "std          0.943604     NaN  \n",
       "min          1.000000     NaN  \n",
       "25%          3.000000     NaN  \n",
       "50%          4.000000     NaN  \n",
       "75%          4.500000     NaN  \n",
       "max          5.000000     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>921 Pearl St</td>\n",
       "      <td>(None, None, 'beer_and_wine', {'touristy': Fal...</td>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>Gastropubs, Food, Beer Gardens, Restaurants, B...</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>(11:0-23:0, 11:0-23:0, 11:0-23:0, 11:0-23:0, 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>40.017544</td>\n",
       "      <td>-105.283348</td>\n",
       "      <td>Oskar Blues Taproom</td>\n",
       "      <td>80302</td>\n",
       "      <td>86</td>\n",
       "      <td>4.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000 NE Airport Way</td>\n",
       "      <td>(None, None, u'beer_and_wine', {'romantic': Fa...</td>\n",
       "      <td>tCbdrRPZA0oiIYSmHG3J0w</td>\n",
       "      <td>Salad, Soup, Sandwiches, Delis, Restaurants, C...</td>\n",
       "      <td>Portland</td>\n",
       "      <td>(5:0-18:0, 5:0-18:0, 5:0-18:0, 5:0-18:0, 5:0-1...</td>\n",
       "      <td>1</td>\n",
       "      <td>45.588906</td>\n",
       "      <td>-122.593331</td>\n",
       "      <td>Flying Elephants at PDX</td>\n",
       "      <td>97218</td>\n",
       "      <td>126</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4720 Hawthorne Ave</td>\n",
       "      <td>(None, None, None, None, None, None, None, Fal...</td>\n",
       "      <td>bvN78flM8NLprQ1a1y5dRg</td>\n",
       "      <td>Antiques, Fashion, Used, Vintage &amp; Consignment...</td>\n",
       "      <td>Portland</td>\n",
       "      <td>(11:0-18:0, None, 11:0-18:0, 11:0-18:0, 11:0-1...</td>\n",
       "      <td>1</td>\n",
       "      <td>45.511907</td>\n",
       "      <td>-122.613693</td>\n",
       "      <td>The Reclaimory</td>\n",
       "      <td>97214</td>\n",
       "      <td>13</td>\n",
       "      <td>4.5</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2566 Enterprise Rd</td>\n",
       "      <td>(None, None, None, None, None, None, None, Non...</td>\n",
       "      <td>oaepsyvc0J17qwi8cfrOWg</td>\n",
       "      <td>Beauty &amp; Spas, Hair Salons</td>\n",
       "      <td>Orange City</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>28.914482</td>\n",
       "      <td>-81.295979</td>\n",
       "      <td>Great Clips</td>\n",
       "      <td>32763</td>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1046 Memorial Dr SE</td>\n",
       "      <td>(None, None, None, None, None, None, None, Non...</td>\n",
       "      <td>PE9uqAjdw0E4-8mjGl3wVA</td>\n",
       "      <td>Gyms, Active Life, Interval Training Gyms, Fit...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>(16:0-19:0, 16:0-19:0, 9:0-11:0, None, 16:0-19...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.747027</td>\n",
       "      <td>-84.353424</td>\n",
       "      <td>Crossfit Terminus</td>\n",
       "      <td>30316</td>\n",
       "      <td>14</td>\n",
       "      <td>4.0</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               address                                         attributes  \\\n",
       "0         921 Pearl St  (None, None, 'beer_and_wine', {'touristy': Fal...   \n",
       "1  7000 NE Airport Way  (None, None, u'beer_and_wine', {'romantic': Fa...   \n",
       "2   4720 Hawthorne Ave  (None, None, None, None, None, None, None, Fal...   \n",
       "3   2566 Enterprise Rd  (None, None, None, None, None, None, None, Non...   \n",
       "4  1046 Memorial Dr SE  (None, None, None, None, None, None, None, Non...   \n",
       "\n",
       "              business_id                                         categories  \\\n",
       "0  6iYb2HFDywm3zjuRg0shjw  Gastropubs, Food, Beer Gardens, Restaurants, B...   \n",
       "1  tCbdrRPZA0oiIYSmHG3J0w  Salad, Soup, Sandwiches, Delis, Restaurants, C...   \n",
       "2  bvN78flM8NLprQ1a1y5dRg  Antiques, Fashion, Used, Vintage & Consignment...   \n",
       "3  oaepsyvc0J17qwi8cfrOWg                         Beauty & Spas, Hair Salons   \n",
       "4  PE9uqAjdw0E4-8mjGl3wVA  Gyms, Active Life, Interval Training Gyms, Fit...   \n",
       "\n",
       "          city                                              hours  is_open  \\\n",
       "0      Boulder  (11:0-23:0, 11:0-23:0, 11:0-23:0, 11:0-23:0, 1...        1   \n",
       "1     Portland  (5:0-18:0, 5:0-18:0, 5:0-18:0, 5:0-18:0, 5:0-1...        1   \n",
       "2     Portland  (11:0-18:0, None, 11:0-18:0, 11:0-18:0, 11:0-1...        1   \n",
       "3  Orange City                                               None        1   \n",
       "4      Atlanta  (16:0-19:0, 16:0-19:0, 9:0-11:0, None, 16:0-19...        1   \n",
       "\n",
       "    latitude   longitude                     name postal_code  review_count  \\\n",
       "0  40.017544 -105.283348      Oskar Blues Taproom       80302            86   \n",
       "1  45.588906 -122.593331  Flying Elephants at PDX       97218           126   \n",
       "2  45.511907 -122.613693           The Reclaimory       97214            13   \n",
       "3  28.914482  -81.295979              Great Clips       32763             8   \n",
       "4  33.747027  -84.353424        Crossfit Terminus       30316            14   \n",
       "\n",
       "   stars state  \n",
       "0    4.0    CO  \n",
       "1    4.0    OR  \n",
       "2    4.5    OR  \n",
       "3    3.0    FL  \n",
       "4    4.0    GA  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "%time\n",
    "df=spark.read.json(f\"{data_path}/yelp_academic_dataset_business.json\")\n",
    "df.createOrReplaceTempView(\"businesses\")\n",
    "df.cache()\n",
    "info(df.toPandas())\n",
    "# kdf=df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:42:29.023012Z",
     "start_time": "2021-12-05T18:42:17.516838Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df=spark.read.json(f\"{data_path}/yelp_academic_dataset_review.json\")\n",
    "df.createOrReplaceTempView(\"reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:42:43.396564Z",
     "start_time": "2021-12-05T18:42:29.025465Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business_id  cool  date  funny  review_id  stars  text  useful  user_id\n",
       "0            0     0     0      0          0      0     0       0        0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/44413132/count-the-number-of-missing-values-in-a-dataframe-spark\n",
    "from pyspark.sql.functions import col,sum\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count empty values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:43:34.848947Z",
     "start_time": "2021-12-05T18:42:43.400000Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business_id  cool  date  funny  review_id  stars  text  useful  user_id\n",
       "0            0     0     0      0          0      0     0       0        0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://towardsdatascience.com/data-prep-with-spark-dataframes-3629478a1041\n",
    "df.select([f.count(f.when(f.isnan(c), c)).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:43:48.469136Z",
     "start_time": "2021-12-05T18:43:34.850745Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cool</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "business_id  0\n",
       "cool         0\n",
       "date         0\n",
       "funny        0\n",
       "review_id    0\n",
       "stars        0\n",
       "text         0\n",
       "useful       0\n",
       "user_id      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in \n",
    "           df.columns]).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:43:48.477356Z",
     "start_time": "2021-12-05T18:43:48.473352Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.filter(f.isnan(\"\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:43:57.331625Z",
     "start_time": "2021-12-05T18:43:48.478737Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8,635,403'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:,}\".format(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:44:09.114444Z",
     "start_time": "2021-12-05T18:43:57.340083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [business_id, cool, date, funny, review_id, stars, text, useful, user_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM reviews WHERE text='' LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:44:09.194087Z",
     "start_time": "2021-12-05T18:44:09.117912Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buF9druCkbuXLX526sGELQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-11 03:34:02</td>\n",
       "      <td>1</td>\n",
       "      <td>lWC-xP3rd6obsecCYsGZRg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Apparently Prides Osteria had a rough summer a...</td>\n",
       "      <td>3</td>\n",
       "      <td>ak0TdVmGKo4pwqdJSTLwWw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RA4V8pr014UyUbDvI-LW2A</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-03 20:38:25</td>\n",
       "      <td>0</td>\n",
       "      <td>8bFej1QE5LXp4O05qjGqXA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>This store is pretty good. Not as great as Wal...</td>\n",
       "      <td>1</td>\n",
       "      <td>YoVfDbnISlW0f7abNQACIg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_sS2LBIGNT5NQb6PD1Vtjw</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-05-28 20:38:06</td>\n",
       "      <td>0</td>\n",
       "      <td>NDhkzczKjLshODbqDoNLSg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I called WVM on the recommendation of a couple...</td>\n",
       "      <td>0</td>\n",
       "      <td>eC5evKn1TWDyHCyQAwguUw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0AzLzHfOJgL7ROwhdww2ew</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-08 02:29:15</td>\n",
       "      <td>1</td>\n",
       "      <td>T5fAqjjFooT4V0OeZyuk1w</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I've stayed at many Marriott and Renaissance M...</td>\n",
       "      <td>1</td>\n",
       "      <td>SFQ1jcnGguO0LYWnbbftAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8zehGz9jnxPqXtOc7KaJxA</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-28 18:05:01</td>\n",
       "      <td>0</td>\n",
       "      <td>sjm_uUcQVxab_EeLCqsYLg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The food is always great here. The service fro...</td>\n",
       "      <td>0</td>\n",
       "      <td>0kA0PAJ8QFMeveQWHFqz2A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  buF9druCkbuXLX526sGELQ     1  2014-10-11 03:34:02      1   \n",
       "1  RA4V8pr014UyUbDvI-LW2A     0  2015-07-03 20:38:25      0   \n",
       "2  _sS2LBIGNT5NQb6PD1Vtjw     0  2013-05-28 20:38:06      0   \n",
       "3  0AzLzHfOJgL7ROwhdww2ew     1  2010-01-08 02:29:15      1   \n",
       "4  8zehGz9jnxPqXtOc7KaJxA     0  2011-07-28 18:05:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  lWC-xP3rd6obsecCYsGZRg    4.0   \n",
       "1  8bFej1QE5LXp4O05qjGqXA    4.0   \n",
       "2  NDhkzczKjLshODbqDoNLSg    5.0   \n",
       "3  T5fAqjjFooT4V0OeZyuk1w    2.0   \n",
       "4  sjm_uUcQVxab_EeLCqsYLg    4.0   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Apparently Prides Osteria had a rough summer a...       3   \n",
       "1  This store is pretty good. Not as great as Wal...       1   \n",
       "2  I called WVM on the recommendation of a couple...       0   \n",
       "3  I've stayed at many Marriott and Renaissance M...       1   \n",
       "4  The food is always great here. The service fro...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  ak0TdVmGKo4pwqdJSTLwWw  \n",
       "1  YoVfDbnISlW0f7abNQACIg  \n",
       "2  eC5evKn1TWDyHCyQAwguUw  \n",
       "3  SFQ1jcnGguO0LYWnbbftAA  \n",
       "4  0kA0PAJ8QFMeveQWHFqz2A  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:44:17.798758Z",
     "start_time": "2021-12-05T18:44:09.200044Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:================================================>       (24 + 4) / 28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15 ms, sys: 224 µs, total: 15.2 ms\n",
      "Wall time: 8.59 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:====================================================>   (26 + 2) / 28]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df=spark.read.json(f\"{data_path}/yelp_academic_dataset_user.json\")\n",
    "users=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:44:21.726056Z",
     "start_time": "2021-12-05T18:44:17.804591Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2,189,457'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/3154460/python-human-readable-large-numbers\n",
    "\n",
    "\"{:,}\".format(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:45:38.520231Z",
     "start_time": "2021-12-05T18:44:21.745255Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>average_stars</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_hot</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_photos</th>\n",
       "      <th>...</th>\n",
       "      <th>cool</th>\n",
       "      <th>elite</th>\n",
       "      <th>fans</th>\n",
       "      <th>friends</th>\n",
       "      <th>funny</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>yelping_since</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>...</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "      <td>2189457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>3.6538161105699274</td>\n",
       "      <td>2.5026232531627706</td>\n",
       "      <td>0.13045700372284086</td>\n",
       "      <td>2.5026232531627706</td>\n",
       "      <td>1.633913340156943</td>\n",
       "      <td>0.06127409672809286</td>\n",
       "      <td>0.2736518689337128</td>\n",
       "      <td>1.2354332603928737</td>\n",
       "      <td>0.9881007025942962</td>\n",
       "      <td>...</td>\n",
       "      <td>20.473540699817352</td>\n",
       "      <td>2014.6331140977982</td>\n",
       "      <td>1.3792186829885218</td>\n",
       "      <td>None</td>\n",
       "      <td>15.39467959407287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.697721398502004</td>\n",
       "      <td>38.056672955897284</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1.1538609330757066</td>\n",
       "      <td>83.63695759997829</td>\n",
       "      <td>10.767452899622612</td>\n",
       "      <td>83.63695759997829</td>\n",
       "      <td>64.40826658665287</td>\n",
       "      <td>9.473195831748187</td>\n",
       "      <td>11.998873844683619</td>\n",
       "      <td>39.82064066188779</td>\n",
       "      <td>87.43188645190922</td>\n",
       "      <td>...</td>\n",
       "      <td>466.82963889748737</td>\n",
       "      <td>3.651799459305662</td>\n",
       "      <td>16.866749723244446</td>\n",
       "      <td>None</td>\n",
       "      <td>353.269747281276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.01254770183907</td>\n",
       "      <td>535.2625345401981</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>---A_S9GsLdfLSURLx6-Dw, jT8iJYsTY8-aD91zSlxfMg...</td>\n",
       "      <td>0</td>\n",
       "      <td>Patrizia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>---2PmXbF47D870stH1jqA</td>\n",
       "      <td>2004-10-12 08:46:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25%</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50%</td>\n",
       "      <td>3.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75%</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max</td>\n",
       "      <td>5.0</td>\n",
       "      <td>46858</td>\n",
       "      <td>13654</td>\n",
       "      <td>46858</td>\n",
       "      <td>25304</td>\n",
       "      <td>12669</td>\n",
       "      <td>13501</td>\n",
       "      <td>38322</td>\n",
       "      <td>82630</td>\n",
       "      <td>...</td>\n",
       "      <td>198451</td>\n",
       "      <td>2019,20,20</td>\n",
       "      <td>12116</td>\n",
       "      <td>zzzcuxFaP_FvdIB-fbP9iA, wgyzGQ9LM8oedgve16uU5g...</td>\n",
       "      <td>172041</td>\n",
       "      <td>ｼﾞｪﾚﾐｰ</td>\n",
       "      <td>15686</td>\n",
       "      <td>204380</td>\n",
       "      <td>zzzqnB-6DlYUbqAPxUxg4A</td>\n",
       "      <td>2021-01-28 15:32:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary       average_stars     compliment_cool      compliment_cute  \\\n",
       "0   count             2189457             2189457              2189457   \n",
       "1    mean  3.6538161105699274  2.5026232531627706  0.13045700372284086   \n",
       "2  stddev  1.1538609330757066   83.63695759997829   10.767452899622612   \n",
       "3     min                 1.0                   0                    0   \n",
       "4     25%                 3.0                   0                    0   \n",
       "5     50%                3.88                   0                    0   \n",
       "6     75%                4.55                   0                    0   \n",
       "7     max                 5.0               46858                13654   \n",
       "\n",
       "     compliment_funny     compliment_hot      compliment_list  \\\n",
       "0             2189457            2189457              2189457   \n",
       "1  2.5026232531627706  1.633913340156943  0.06127409672809286   \n",
       "2   83.63695759997829  64.40826658665287    9.473195831748187   \n",
       "3                   0                  0                    0   \n",
       "4                   0                  0                    0   \n",
       "5                   0                  0                    0   \n",
       "6                   0                  0                    0   \n",
       "7               46858              25304                12669   \n",
       "\n",
       "      compliment_more     compliment_note   compliment_photos  ...  \\\n",
       "0             2189457             2189457             2189457  ...   \n",
       "1  0.2736518689337128  1.2354332603928737  0.9881007025942962  ...   \n",
       "2  11.998873844683619   39.82064066188779   87.43188645190922  ...   \n",
       "3                   0                   0                   0  ...   \n",
       "4                   0                   0                   0  ...   \n",
       "5                   0                   0                   0  ...   \n",
       "6                   0                   0                   0  ...   \n",
       "7               13501               38322               82630  ...   \n",
       "\n",
       "                 cool               elite                fans  \\\n",
       "0             2189457             2189457             2189457   \n",
       "1  20.473540699817352  2014.6331140977982  1.3792186829885218   \n",
       "2  466.82963889748737   3.651799459305662  16.866749723244446   \n",
       "3                   0                                       0   \n",
       "4                   0              2012.0                   0   \n",
       "5                   0              2016.0                   0   \n",
       "6                   3              2018.0                   0   \n",
       "7              198451          2019,20,20               12116   \n",
       "\n",
       "                                             friends              funny  \\\n",
       "0                                            2189457            2189457   \n",
       "1                                               None  15.39467959407287   \n",
       "2                                               None   353.269747281276   \n",
       "3  ---A_S9GsLdfLSURLx6-Dw, jT8iJYsTY8-aD91zSlxfMg...                  0   \n",
       "4                                               None                  0   \n",
       "5                                               None                  0   \n",
       "6                                               None                  3   \n",
       "7  zzzcuxFaP_FvdIB-fbP9iA, wgyzGQ9LM8oedgve16uU5g...             172041   \n",
       "\n",
       "         name        review_count              useful                 user_id  \\\n",
       "0     2189457             2189457             2189457                 2189457   \n",
       "1         NaN  21.697721398502004  38.056672955897284                    None   \n",
       "2         NaN   76.01254770183907   535.2625345401981                    None   \n",
       "3    Patrizia                   0                   0  ---2PmXbF47D870stH1jqA   \n",
       "4        98.0                   2                   1                    None   \n",
       "5         NaN                   5                   3                    None   \n",
       "6         NaN                  15                  13                    None   \n",
       "7      ｼﾞｪﾚﾐｰ               15686              204380  zzzqnB-6DlYUbqAPxUxg4A   \n",
       "\n",
       "         yelping_since  \n",
       "0              2189457  \n",
       "1                 None  \n",
       "2                 None  \n",
       "3  2004-10-12 08:46:11  \n",
       "4                 None  \n",
       "5                 None  \n",
       "6                 None  \n",
       "7  2021-01-28 15:32:55  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.summary().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:45:38.650267Z",
     "start_time": "2021-12-05T18:45:38.546324Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_stars</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_hot</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_photos</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>...</th>\n",
       "      <th>cool</th>\n",
       "      <th>elite</th>\n",
       "      <th>fans</th>\n",
       "      <th>friends</th>\n",
       "      <th>funny</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>yelping_since</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.85</td>\n",
       "      <td>2541</td>\n",
       "      <td>361</td>\n",
       "      <td>2541</td>\n",
       "      <td>1710</td>\n",
       "      <td>147</td>\n",
       "      <td>163</td>\n",
       "      <td>1212</td>\n",
       "      <td>323</td>\n",
       "      <td>5691</td>\n",
       "      <td>...</td>\n",
       "      <td>11291</td>\n",
       "      <td>2006,2007,2008,2009,2010,2011,2012,2013,2014</td>\n",
       "      <td>1357</td>\n",
       "      <td>xBDpTUbai0DXrvxCe3X16Q, 7GPNBO496aecrjJfW6UWtg...</td>\n",
       "      <td>10030</td>\n",
       "      <td>Jane</td>\n",
       "      <td>1220</td>\n",
       "      <td>15038</td>\n",
       "      <td>q_QQ5kBBwlCcbL1s4NVK3g</td>\n",
       "      <td>2005-03-14 20:26:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.09</td>\n",
       "      <td>2205</td>\n",
       "      <td>232</td>\n",
       "      <td>2205</td>\n",
       "      <td>1632</td>\n",
       "      <td>96</td>\n",
       "      <td>87</td>\n",
       "      <td>1187</td>\n",
       "      <td>294</td>\n",
       "      <td>3293</td>\n",
       "      <td>...</td>\n",
       "      <td>18046</td>\n",
       "      <td>2007,2008,2009,2010,2011,2012,2013,2014,2015,2...</td>\n",
       "      <td>1025</td>\n",
       "      <td>XPzYf9_mwG2eXYP2BAGSTA, 2LooM5dcIk2o01nftYdPIg...</td>\n",
       "      <td>10289</td>\n",
       "      <td>Gabi</td>\n",
       "      <td>2136</td>\n",
       "      <td>21272</td>\n",
       "      <td>dIIKEfOgo0KqUfGQvGikPg</td>\n",
       "      <td>2007-08-10 19:01:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.76</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>2010,2011</td>\n",
       "      <td>16</td>\n",
       "      <td>GfB6sC4NJQvSI2ewbQrDNA, jhZtzZNNZJOU2YSZ6jPlXQ...</td>\n",
       "      <td>128</td>\n",
       "      <td>Jason</td>\n",
       "      <td>119</td>\n",
       "      <td>188</td>\n",
       "      <td>D6ErcUnFALnCQN4b1W_TlA</td>\n",
       "      <td>2007-02-07 15:47:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.77</td>\n",
       "      <td>1566</td>\n",
       "      <td>219</td>\n",
       "      <td>1566</td>\n",
       "      <td>1180</td>\n",
       "      <td>90</td>\n",
       "      <td>129</td>\n",
       "      <td>1120</td>\n",
       "      <td>326</td>\n",
       "      <td>4510</td>\n",
       "      <td>...</td>\n",
       "      <td>4035</td>\n",
       "      <td>2009,2010,2011,2012,2013,2014</td>\n",
       "      <td>420</td>\n",
       "      <td>HQZPQhKMwRAyS6BCselVWQ, kP2U1s_sjQfHO9grxiyDTA...</td>\n",
       "      <td>4722</td>\n",
       "      <td>Kat</td>\n",
       "      <td>987</td>\n",
       "      <td>7234</td>\n",
       "      <td>JnPIjvC0cmooNDfsa9BmXg</td>\n",
       "      <td>2009-02-09 16:14:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.72</td>\n",
       "      <td>310</td>\n",
       "      <td>16</td>\n",
       "      <td>310</td>\n",
       "      <td>248</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>77</td>\n",
       "      <td>44</td>\n",
       "      <td>131</td>\n",
       "      <td>...</td>\n",
       "      <td>1124</td>\n",
       "      <td>2009,2010,2011</td>\n",
       "      <td>47</td>\n",
       "      <td>-Q88pZUcrfN0BLBDp-bkAQ, etPn4Pv1Gc4cRZjRgB_BOw...</td>\n",
       "      <td>727</td>\n",
       "      <td>Christine</td>\n",
       "      <td>495</td>\n",
       "      <td>1577</td>\n",
       "      <td>37Hc8hr3cw0iHLoPzLK6Ow</td>\n",
       "      <td>2008-03-03 04:57:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   average_stars  compliment_cool  compliment_cute  compliment_funny  \\\n",
       "0           3.85             2541              361              2541   \n",
       "1           4.09             2205              232              2205   \n",
       "2           3.76               31                0                31   \n",
       "3           3.77             1566              219              1566   \n",
       "4           3.72              310               16               310   \n",
       "\n",
       "   compliment_hot  compliment_list  compliment_more  compliment_note  \\\n",
       "0            1710              147              163             1212   \n",
       "1            1632               96               87             1187   \n",
       "2              22                0                1                5   \n",
       "3            1180               90              129             1120   \n",
       "4             248               15               19               77   \n",
       "\n",
       "   compliment_photos  compliment_plain  ...   cool  \\\n",
       "0                323              5691  ...  11291   \n",
       "1                294              3293  ...  18046   \n",
       "2                  1                20  ...    130   \n",
       "3                326              4510  ...   4035   \n",
       "4                 44               131  ...   1124   \n",
       "\n",
       "                                               elite  fans  \\\n",
       "0       2006,2007,2008,2009,2010,2011,2012,2013,2014  1357   \n",
       "1  2007,2008,2009,2010,2011,2012,2013,2014,2015,2...  1025   \n",
       "2                                          2010,2011    16   \n",
       "3                      2009,2010,2011,2012,2013,2014   420   \n",
       "4                                     2009,2010,2011    47   \n",
       "\n",
       "                                             friends  funny       name  \\\n",
       "0  xBDpTUbai0DXrvxCe3X16Q, 7GPNBO496aecrjJfW6UWtg...  10030       Jane   \n",
       "1  XPzYf9_mwG2eXYP2BAGSTA, 2LooM5dcIk2o01nftYdPIg...  10289       Gabi   \n",
       "2  GfB6sC4NJQvSI2ewbQrDNA, jhZtzZNNZJOU2YSZ6jPlXQ...    128      Jason   \n",
       "3  HQZPQhKMwRAyS6BCselVWQ, kP2U1s_sjQfHO9grxiyDTA...   4722        Kat   \n",
       "4  -Q88pZUcrfN0BLBDp-bkAQ, etPn4Pv1Gc4cRZjRgB_BOw...    727  Christine   \n",
       "\n",
       "   review_count useful                 user_id        yelping_since  \n",
       "0          1220  15038  q_QQ5kBBwlCcbL1s4NVK3g  2005-03-14 20:26:35  \n",
       "1          2136  21272  dIIKEfOgo0KqUfGQvGikPg  2007-08-10 19:01:51  \n",
       "2           119    188  D6ErcUnFALnCQN4b1W_TlA  2007-02-07 15:47:53  \n",
       "3           987   7234  JnPIjvC0cmooNDfsa9BmXg  2009-02-09 16:14:29  \n",
       "4           495   1577  37Hc8hr3cw0iHLoPzLK6Ow  2008-03-03 04:57:05  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T19:18:07.755437Z",
     "start_time": "2021-12-14T19:17:54.770264Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(f\"{data_path}/yelp_academic_dataset_checkin.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T19:19:19.463536Z",
     "start_time": "2021-12-14T19:19:19.381177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3150103"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.date.str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T18:40:49.586700Z",
     "start_time": "2021-10-23T18:40:49.577881Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:45:38.657477Z",
     "start_time": "2021-12-05T18:45:38.653085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:45:38.672852Z",
     "start_time": "2021-12-05T18:45:38.668776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-07T19:31:15.579Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "for path in glob.glob(f\"{data_path}/*.json\"):\n",
    "    print(path)\n",
    "    print(path[path.rindex(\"/\")+1:path.rindex(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T20:04:27.662660Z",
     "start_time": "2021-12-12T19:59:10.730851Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/12 21:59:12 WARN Utils: Your hostname, babkamen-Lenovo resolves to a loopback address: 127.0.1.1; using 192.168.0.133 instead (on interface wlp3s0)\n",
      "21/12/12 21:59:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/babkamen/Documents/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/12 21:59:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.port', '34593'), ('spark.master', 'local[7]'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.app.startTime', '1639339153260'), ('spark.driver.host', '192.168.0.133'), ('spark.submit.pyFiles', ''), ('spark.executor.id', 'driver'), ('spark.submit.deployMode', 'client'), ('spark.sql.warehouse.dir', 'file:/home/babkamen/git/udacity-big-data-engineer-nanodegree/capstone-project/spark-warehouse'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'pyspark-shell'), ('spark.app.id', 'local-1639339154426')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===============>                                         (14 + 7) / 52]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 10076 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o81.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 3.0 failed 1 times, most recent failure: Lost task 24.0 in stage 3.0 (TID 132) (192.168.0.133 executor driver): java.sql.BatchUpdateException: Batch entry 512 INSERT INTO reviews (\"business_id\",\"cool\",\"date\",\"funny\",\"review_id\",\"stars\",\"text\",\"useful\",\"user_id\") VALUES ('qCyDatIL1MNeSo12WJEZjg',3,'2007-11-13 22:39:38',5,'T2vUdvUZ_dY76S4xpmxz6A',5.0,'I''ve recently become the Laurelhurst''s biggest fan. Pleased to meet you.\n\nFor years have I shirked going to the pub theaters. They seemed cheesy and gimmicky...I''m judgmental and hardheaded, I suppose.  \nIt was foolishness! Beer and movies?! In the same building? I was living a total lie.\nThe Laurelhurst is the jam.  Yes, beer and movies.  Also...they''re good movies, you guys. Movies I actually want to see. Sorry, I''m not much of a heavy-handed-overwrought-blockbuster gal. Old and new. For young and old. So nice!\nThere''s also pizza. I really like pizza.\n\nAnd it costs me $3.  I can totally and completely afford that.\n\nI watched Purple Rain here. I like that. Period!',2,'h7DOfAaP3b_6lp__qVxSiA') was aborted: ERROR: could not extend file \"base/25088/443761.2\": wrote only 4096 of 8192 bytes at block 324405\n  Hint: Check free disk space.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2365)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2097)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1454)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1479)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:544)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:881)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:904)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1634)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:723)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:890)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.postgresql.util.PSQLException: ERROR: could not extend file \"base/25088/443761.2\": wrote only 4096 of 8192 bytes at block 324405\n  Hint: Check free disk space.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2674)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2364)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:888)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:745)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.sql.BatchUpdateException: Batch entry 512 INSERT INTO reviews (\"business_id\",\"cool\",\"date\",\"funny\",\"review_id\",\"stars\",\"text\",\"useful\",\"user_id\") VALUES ('qCyDatIL1MNeSo12WJEZjg',3,'2007-11-13 22:39:38',5,'T2vUdvUZ_dY76S4xpmxz6A',5.0,'I''ve recently become the Laurelhurst''s biggest fan. Pleased to meet you.\n\nFor years have I shirked going to the pub theaters. They seemed cheesy and gimmicky...I''m judgmental and hardheaded, I suppose.  \nIt was foolishness! Beer and movies?! In the same building? I was living a total lie.\nThe Laurelhurst is the jam.  Yes, beer and movies.  Also...they''re good movies, you guys. Movies I actually want to see. Sorry, I''m not much of a heavy-handed-overwrought-blockbuster gal. Old and new. For young and old. So nice!\nThere''s also pizza. I really like pizza.\n\nAnd it costs me $3.  I can totally and completely afford that.\n\nI watched Purple Rain here. I like that. Period!',2,'h7DOfAaP3b_6lp__qVxSiA') was aborted: ERROR: could not extend file \"base/25088/443761.2\": wrote only 4096 of 8192 bytes at block 324405\n  Hint: Check free disk space.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2365)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2097)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1454)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1479)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:544)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:881)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:904)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1634)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:723)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:890)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: could not extend file \"base/25088/443761.2\": wrote only 4096 of 8192 bytes at block 324405\n  Hint: Check free disk space.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2674)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2364)\n\t... 21 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52649/2867959095.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{data_path}/yelp_academic_dataset_user.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"users\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{data_path}/yelp_academic_dataset_review.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"reviews\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{data_path}/yelp_academic_dataset_business.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attributes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/udacity-big-data-engineer-nanodegree/capstone-project/venv/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/udacity-big-data-engineer-nanodegree/capstone-project/venv/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o81.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 3.0 failed 1 times, most recent failure: Lost task 24.0 in stage 3.0 (TID 132) (192.168.0.133 executor driver): java.sql.BatchUpdateException: Batch entry 512 INSERT INTO reviews (\"business_id\",\"cool\",\"date\",\"funny\",\"review_id\",\"stars\",\"text\",\"useful\",\"user_id\") VALUES ('qCyDatIL1MNeSo12WJEZjg',3,'2007-11-13 22:39:38',5,'T2vUdvUZ_dY76S4xpmxz6A',5.0,'I''ve recently become the Laurelhurst''s biggest fan. Pleased to meet you.\n\nFor years have I shirked going to the pub theaters. They seemed cheesy and gimmicky...I''m judgmental and hardheaded, I suppose.  \nIt was foolishness! Beer and movies?! In the same building? I was living a total lie.\nThe Laurelhurst is the jam.  Yes, beer and movies.  Also...they''re good movies, you guys. Movies I actually want to see. Sorry, I''m not much of a heavy-handed-overwrought-blockbuster gal. Old and new. For young and old. So nice!\nThere''s also pizza. I really like pizza.\n\nAnd it costs me $3.  I can totally and completely afford that.\n\nI watched Purple Rain here. I like that. Period!',2,'h7DOfAaP3b_6lp__qVxSiA') was aborted: ERROR: could not extend file \"base/25088/443761.2\": wrote only 4096 of 8192 bytes at block 324405\n  Hint: Check free disk space.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2365)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2097)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1454)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1479)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:544)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:881)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:904)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1634)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:723)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:890)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.postgresql.util.PSQLException: ERROR: could not extend file \"base/25088/443761.2\": wrote only 4096 of 8192 bytes at block 324405\n  Hint: Check free disk space.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2674)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2364)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:888)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:745)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.sql.BatchUpdateException: Batch entry 512 INSERT INTO reviews (\"business_id\",\"cool\",\"date\",\"funny\",\"review_id\",\"stars\",\"text\",\"useful\",\"user_id\") VALUES ('qCyDatIL1MNeSo12WJEZjg',3,'2007-11-13 22:39:38',5,'T2vUdvUZ_dY76S4xpmxz6A',5.0,'I''ve recently become the Laurelhurst''s biggest fan. Pleased to meet you.\n\nFor years have I shirked going to the pub theaters. They seemed cheesy and gimmicky...I''m judgmental and hardheaded, I suppose.  \nIt was foolishness! Beer and movies?! In the same building? I was living a total lie.\nThe Laurelhurst is the jam.  Yes, beer and movies.  Also...they''re good movies, you guys. Movies I actually want to see. Sorry, I''m not much of a heavy-handed-overwrought-blockbuster gal. Old and new. For young and old. So nice!\nThere''s also pizza. I really like pizza.\n\nAnd it costs me $3.  I can totally and completely afford that.\n\nI watched Purple Rain here. I like that. Period!',2,'h7DOfAaP3b_6lp__qVxSiA') was aborted: ERROR: could not extend file \"base/25088/443761.2\": wrote only 4096 of 8192 bytes at block 324405\n  Hint: Check free disk space.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2365)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2097)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1454)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1479)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:544)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:881)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:904)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1634)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:723)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:890)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: could not extend file \"base/25088/443761.2\": wrote only 4096 of 8192 bytes at block 324405\n  Hint: Check free disk space.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2674)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2364)\n\t... 21 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# spark.stop()\n",
    "\n",
    "data_path=\"./data/yelp\"\n",
    "spark = SparkSession.builder.master(\"local[7]\").getOrCreate()\n",
    "\n",
    "\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "mode = \"overwrite\"\n",
    "url = \"jdbc:postgresql://127.0.0.1:5432/dwh\"\n",
    "properties = {\"user\": \"airflow\",\"password\": \"airflow\",\"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# for path in glob.glob(f\"{data_path}/*.json\"):\n",
    "#     tbl=path[path.rindex(\"/\")+1:path.rindex(\".\")]\n",
    "#     spark.read.json(path).write.jdbc(url=url, table=tbl, mode=mode, properties=properties)\n",
    "\n",
    "\n",
    "\n",
    "df = spark.read.json(f\"{data_path}/yelp_academic_dataset_checkin.json\")\n",
    "\n",
    "\n",
    "spark.read.json(f\"{data_path}/yelp_academic_dataset_user.json\").write.jdbc(url=url, table=\"users\", mode=mode, properties=properties)\n",
    "\n",
    "spark.read.json(f\"{data_path}/yelp_academic_dataset_review.json\").write.jdbc(url=url, table=\"reviews\", mode=mode, properties=properties)\n",
    "\n",
    "df= spark.read.json(f\"{data_path}/yelp_academic_dataset_business.json\").drop(\"attributes\")\n",
    "df.printSchema()\n",
    "df.drop('hours').write.jdbc(url=url, table=\"businesses\", mode=mode, properties=properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:15:42.029950Z",
     "start_time": "2021-12-15T19:15:32.795440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/15 21:15:34 WARN Utils: Your hostname, babkamen-Lenovo resolves to a loopback address: 127.0.1.1; using 192.168.0.133 instead (on interface wlp3s0)\n",
      "21/12/15 21:15:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/babkamen/Documents/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/15 21:15:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.startTime', '1639595735187'), ('spark.master', 'local[7]'), ('spark.rdd.compress', 'True'), ('spark.app.id', 'local-1639595736239'), ('spark.serializer.objectStreamReset', '100'), ('spark.driver.host', '192.168.0.133'), ('spark.submit.pyFiles', ''), ('spark.executor.id', 'driver'), ('spark.submit.deployMode', 'client'), ('spark.sql.warehouse.dir', 'file:/home/babkamen/git/udacity-big-data-engineer-nanodegree/capstone-project/spark-warehouse'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'pyspark-shell'), ('spark.driver.port', '42539')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# spark.stop()\n",
    "\n",
    "data_path=\"./data/yelp\"\n",
    "spark = SparkSession.builder.master(\"local[7]\").getOrCreate()\n",
    "\n",
    "\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "mode = \"overwrite\"\n",
    "url = \"jdbc:postgresql://127.0.0.1:5432/dwh\"\n",
    "properties = {\"user\": \"airflow\",\"password\": \"airflow\",\"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# for path in glob.glob(f\"{data_path}/*.json\"):\n",
    "#     tbl=path[path.rindex(\"/\")+1:path.rindex(\".\")]\n",
    "#     spark.read.json(path).write.jdbc(url=url, table=tbl, mode=mode, properties=properties)\n",
    "\n",
    "\n",
    "\n",
    "df = spark.read.json(f\"{data_path}/yelp_academic_dataset_checkin.json\")\n",
    "from pyspark.sql.functions import split,explode, ltrim\n",
    "# df.show()\n",
    "df2= df.selectExpr(\"business_id\", \"explode(split(date,',')) as date\") .selectExpr(\"business_id\", \"ltrim(date) as date\")\n",
    "df2.show(20, False) \n",
    "df2.coalesce(1).write.format('json').mode(\"overwrite\").save('./check_ins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-19T18:38:03.124353Z",
     "start_time": "2021-12-19T18:38:03.100923Z"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager().update('notebook', {'limit_output': 50000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-19T19:00:45.752103Z",
     "start_time": "2021-12-19T18:57:14.567190Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|17971548|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|             user_id|           friend_id|             user_id|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Wo_ASbM3pzJ4r1Qf6...|--WdohYHcU0CtA6gB...|--WdohYHcU0CtA6gB...|\n",
      "|fyQ_PV1z7TsUTOUKA...|--WdohYHcU0CtA6gB...|--WdohYHcU0CtA6gB...|\n",
      "|0j-MgFIeN4PAW0Zab...|--WdohYHcU0CtA6gB...|--WdohYHcU0CtA6gB...|\n",
      "|LgqaNO1mXwWncSfmS...|--WdohYHcU0CtA6gB...|--WdohYHcU0CtA6gB...|\n",
      "|JJukAvHwwV_tVycod...|--WdohYHcU0CtA6gB...|--WdohYHcU0CtA6gB...|\n",
      "|jsAyVCvJJm6m7FNPR...|--WdohYHcU0CtA6gB...|--WdohYHcU0CtA6gB...|\n",
      "|ag7t0ohT-T5rkVkGf...|--WdohYHcU0CtA6gB...|--WdohYHcU0CtA6gB...|\n",
      "|bMlZPO4ifuwSR-clu...|--WdohYHcU0CtA6gB...|--WdohYHcU0CtA6gB...|\n",
      "|xqJrrN26uzjbMgPip...|--WdohYHcU0CtA6gB...|--WdohYHcU0CtA6gB...|\n",
      "|wpr6gw4FnptTrk1Ce...|--WdohYHcU0CtA6gB...|--WdohYHcU0CtA6gB...|\n",
      "|-XZOz3ViFET3IZFRG...|--WdohYHcU0CtA6gB...|--WdohYHcU0CtA6gB...|\n",
      "|Ad3n5RVoy1uYuzwB2...|--z9XJZF0T2r7aIsZ...|--z9XJZF0T2r7aIsZ...|\n",
      "|es9BFjQvNnhCfRR5F...|-0GvgpqsBQCfAFxCI...|-0GvgpqsBQCfAFxCI...|\n",
      "|3W-bcAXDYAo89UHtt...|-0GvgpqsBQCfAFxCI...|-0GvgpqsBQCfAFxCI...|\n",
      "|xm8l-RyN5pFejC8k1...|-0GvgpqsBQCfAFxCI...|-0GvgpqsBQCfAFxCI...|\n",
      "|gLyR_6ysEJuZbR4jn...|-0GvgpqsBQCfAFxCI...|-0GvgpqsBQCfAFxCI...|\n",
      "|v3UTyRvGjoNhk2dry...|-0GvgpqsBQCfAFxCI...|-0GvgpqsBQCfAFxCI...|\n",
      "|6g_wek4zlewvsJt64...|-0GvgpqsBQCfAFxCI...|-0GvgpqsBQCfAFxCI...|\n",
      "|aUaNJfY9az71_oMP4...|-0GvgpqsBQCfAFxCI...|-0GvgpqsBQCfAFxCI...|\n",
      "|3GAe3v9pGZmTyi_zV...|-0GvgpqsBQCfAFxCI...|-0GvgpqsBQCfAFxCI...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# spark.stop()\n",
    "from pyspark.sql.functions import col\n",
    "data_path=\"./data/yelp\"\n",
    "spark = SparkSession.builder.master(\"local[7]\").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "df = spark.read.json(f\"{data_path}/yelp_academic_dataset_user.json\")\n",
    "\n",
    "# from pyspark.sql.functions import split,explode, ltrim\n",
    "# # df.show()\n",
    "df_users=df.selectExpr(\"user_id\").alias(\"u\")\n",
    "df_friends= df.filter(df.friends!=\"None\").selectExpr(\"user_id\", \"explode(split(friends,',')) as friend_id\") .selectExpr(\"user_id\", \"ltrim(friend_id) as friend_id\").alias(\"f\")\n",
    "df_friends = df_friends.join(df_users, col(\"f.friend_id\") == col(\"u.user_id\")).drop(\"u.user_id\")\n",
    "df_friends.createOrReplaceTempView(\"users\")\n",
    "spark.sql(\"SELECT COUNT(*) FROM users\").show()\n",
    "df_friends.show()\n",
    "\n",
    "# df2.show(20, False) \n",
    "# df2.coalesce(1).write.format('json').mode(\"overwrite\").save('./friends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:45:40.270987Z",
     "start_time": "2021-12-05T18:45:40.270969Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
