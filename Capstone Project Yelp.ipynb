{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:22.731939Z",
     "start_time": "2021-11-21T09:57:22.727008Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "In this project we will be building analytics database using [Yelp dataset](https://www.yelp.com/dataset). This analytics table can be used to answer questions like: \n",
    "- How different discount, customer relationship programs, etc on reviews. So basically data analyst should be able to run a query and see how the reviews score changes during program time vs other times\n",
    "- Have ability to fitler out negative(less than average review score for given business)reviews and find top used words/phrases\n",
    "- Find top users that provided most value to the business using reviews/tips\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "Dataset contains a number of of newline delimeted json files.\n",
    "\n",
    "#### Dataset description \n",
    "From [the dataset description](https://www.yelp.com/dataset/documentation/main)\n",
    "\n",
    "Each file is composed of a single object type, one JSON-object per-line.\n",
    "\n",
    "Take a look at some examples to get you started: https://github.com/Yelp/dataset-examples.\n",
    "\n",
    "Note: the follow examples contain inline comments, which are technically not valid JSON. This is done here to simplify the documentation and explaining the structure, the JSON files you download will not contain any comments and will be fully valid JSON.\n",
    "\n",
    "Sources:\n",
    "\n",
    "- business.json - Contains business data including location data, attributes, and categories.\n",
    "\n",
    "```json \n",
    "{\n",
    "    // string, 22 character unique string business id\n",
    "    \"business_id\": \"tnhfDv5Il8EaGSXZGiuQGg\",\n",
    "\n",
    "    // string, the business's name\n",
    "    \"name\": \"Garaje\",\n",
    "\n",
    "    // string, the full address of the business\n",
    "    \"address\": \"475 3rd St\",\n",
    "\n",
    "    // string, the city\n",
    "    \"city\": \"San Francisco\",\n",
    "\n",
    "    // string, 2 character state code, if applicable\n",
    "    \"state\": \"CA\",\n",
    "\n",
    "    // string, the postal code\n",
    "    \"postal code\": \"94107\",\n",
    "\n",
    "    // float, latitude\n",
    "    \"latitude\": 37.7817529521,\n",
    "\n",
    "    // float, longitude\n",
    "    \"longitude\": -122.39612197,\n",
    "\n",
    "    // float, star rating, rounded to half-stars\n",
    "    \"stars\": 4.5,\n",
    "\n",
    "    // integer, number of reviews\n",
    "    \"review_count\": 1198,\n",
    "\n",
    "    //TODO: convert to boolean\n",
    "\n",
    "    // integer, 0 or 1 for closed or open, respectively\n",
    "    \"is_open\": 1,\n",
    "\n",
    "    //TODO: do we need this?\n",
    "    \n",
    "    // object, business attributes to values. note: some attribute values might be objects\n",
    "    \"attributes\": {\n",
    "        \"RestaurantsTakeOut\": true,\n",
    "        \"BusinessParking\": {\n",
    "            \"garage\": false,\n",
    "            \"street\": true,\n",
    "            \"validated\": false,\n",
    "            \"lot\": false,\n",
    "            \"valet\": false\n",
    "        },\n",
    "    },\n",
    "\n",
    "    // an array of strings of business categories\n",
    "    \"categories\": [\n",
    "        \"Mexican\",\n",
    "        \"Burgers\",\n",
    "        \"Gastropubs\"\n",
    "    ],\n",
    "\n",
    "    // an object of key day to value hours, hours are using a 24hr clock\n",
    "    \"hours\": {\n",
    "        \"Monday\": \"10:00-21:00\",\n",
    "        \"Tuesday\": \"10:00-21:00\",\n",
    "        \"Friday\": \"10:00-21:00\",\n",
    "        \"Wednesday\": \"10:00-21:00\",\n",
    "        \"Thursday\": \"10:00-21:00\",\n",
    "        \"Sunday\": \"11:00-18:00\",\n",
    "        \"Saturday\": \"10:00-21:00\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "- review.json - Contains full review text data including the user_id that wrote the review and the business_id the review is written for.\n",
    "```json\n",
    "{\n",
    "    // string, 22 character unique review id\n",
    "    \"review_id\": \"zdSx_SD6obEhz9VrW9uAWA\",\n",
    "\n",
    "    // string, 22 character unique user id, maps to the user in user.json\n",
    "    \"user_id\": \"Ha3iJu77CxlrFm-vQRs_8g\",\n",
    "\n",
    "    // string, 22 character business id, maps to business in business.json\n",
    "    \"business_id\": \"tnhfDv5Il8EaGSXZGiuQGg\",\n",
    "\n",
    "    // integer, star rating\n",
    "    \"stars\": 4,\n",
    "\n",
    "    // string, date formatted YYYY-MM-DD\n",
    "    \"date\": \"2016-03-09\",\n",
    "\n",
    "    // string, the review itself\n",
    "    \"text\": \"Great place to hang out after work: the prices are decent, and the ambience is fun. It's a bit loud, but very lively. The staff is friendly, and the food is good. They have a good selection of drinks.\",\n",
    "\n",
    "    //TODO: check min max for next values\n",
    "    \n",
    "    // integer, number of useful votes received\n",
    "    \"useful\": 0,\n",
    "\n",
    "    // integer, number of funny votes received\n",
    "    \"funny\": 0,\n",
    "\n",
    "    // integer, number of cool votes received\n",
    "    \"cool\": 0\n",
    "}\n",
    "```\n",
    "- user.json - User data including the user's friend mapping and all the metadata associated with the user.\n",
    "```json\n",
    "{\n",
    "    // string, 22 character unique user id, maps to the user in user.json\n",
    "    \"user_id\": \"Ha3iJu77CxlrFm-vQRs_8g\",\n",
    "\n",
    "    // string, the user's first name\n",
    "    \"name\": \"Sebastien\",\n",
    "\n",
    "    // integer, the number of reviews they've written\n",
    "    \"review_count\": 56,\n",
    "\n",
    "    // string, when the user joined Yelp, formatted like YYYY-MM-DD\n",
    "    \"yelping_since\": \"2011-01-01\",\n",
    "\n",
    "    // array of strings, an array of the user's friend as user_ids\n",
    "    \"friends\": [\n",
    "        \"wqoXYLWmpkEH0YvTmHBsJQ\",\n",
    "        \"KUXLLiJGrjtSsapmxmpvTA\",\n",
    "        \"6e9rJKQC3n0RSKyHLViL-Q\"\n",
    "    ],\n",
    "\n",
    "    // integer, number of useful votes sent by the user\n",
    "    \"useful\": 21,\n",
    "\n",
    "    // integer, number of funny votes sent by the user\n",
    "    \"funny\": 88,\n",
    "\n",
    "    // integer, number of cool votes sent by the user\n",
    "    \"cool\": 15,\n",
    "\n",
    "    // integer, number of fans the user has\n",
    "    \"fans\": 1032,\n",
    "\n",
    "    // array of integers, the years the user was elite\n",
    "    \"elite\": [\n",
    "        2012,\n",
    "        2013\n",
    "    ],\n",
    "\n",
    "    //TODO: do we need this?\n",
    "    \n",
    "    // float, average rating of all reviews\n",
    "    \"average_stars\": 4.31,\n",
    "\n",
    "    // integer, number of hot compliments received by the user\n",
    "    \"compliment_hot\": 339,\n",
    "\n",
    "    // integer, number of more compliments received by the user\n",
    "    \"compliment_more\": 668,\n",
    "\n",
    "    // integer, number of profile compliments received by the user\n",
    "    \"compliment_profile\": 42,\n",
    "\n",
    "    // integer, number of cute compliments received by the user\n",
    "    \"compliment_cute\": 62,\n",
    "\n",
    "    // integer, number of list compliments received by the user\n",
    "    \"compliment_list\": 37,\n",
    "\n",
    "    // integer, number of note compliments received by the user\n",
    "    \"compliment_note\": 356,\n",
    "\n",
    "    // integer, number of plain compliments received by the user\n",
    "    \"compliment_plain\": 68,\n",
    "\n",
    "    // integer, number of cool compliments received by the user\n",
    "    \"compliment_cool\": 91,\n",
    "\n",
    "    // integer, number of funny compliments received by the user\n",
    "    \"compliment_funny\": 99,\n",
    "\n",
    "    // integer, number of writer compliments received by the user\n",
    "    \"compliment_writer\": 95,\n",
    "\n",
    "    // integer, number of photo compliments received by the user\n",
    "    \"compliment_photos\": 50\n",
    "}\n",
    "```\n",
    "\n",
    "- checkin.json - Checkins on a business.\n",
    "```json\n",
    "{\n",
    "    // string, 22 character business id, maps to business in business.json\n",
    "    \"business_id\": \"tnhfDv5Il8EaGSXZGiuQGg\"\n",
    "\n",
    "    // string which is a comma-separated list of timestamps for each checkin, each with format YYYY-MM-DD HH:MM:SS\n",
    "    \"date\": \"2016-04-26 19:49:16, 2016-08-30 18:36:57, 2016-10-15 02:45:18, 2016-11-18 01:54:50, 2017-04-20 18:39:06, 2017-05-03 17:58:02\"\n",
    "}\n",
    "```\n",
    "- tip.json - Tips written by a user on a business. Tips are shorter than reviews and tend to convey quick suggestions.\n",
    "```json\n",
    "{\n",
    "    // string, text of the tip\n",
    "    \"text\": \"Secret menu - fried chicken sando is da bombbbbbb Their zapatos are good too.\",\n",
    "\n",
    "    // string, when the tip was written, formatted like YYYY-MM-DD\n",
    "    \"date\": \"2013-09-20\",\n",
    "\n",
    "    // integer, how many compliments it has\n",
    "    \"compliment_count\": 172,\n",
    "\n",
    "    // string, 22 character business id, maps to business in business.json\n",
    "    \"business_id\": \"tnhfDv5Il8EaGSXZGiuQGg\",\n",
    "\n",
    "    // string, 22 character unique user id, maps to the user in user.json\n",
    "    \"user_id\": \"49JhAJh8vSQ-vM4Aourl0g\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:22.740377Z",
     "start_time": "2021-11-21T09:57:22.735865Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:58:24.259362Z",
     "start_time": "2021-11-21T09:58:24.250415Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "%time\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "\n",
    "prev = spark.conf.get(\"spark.sql.execution.arrow.enabled\")  # Keep its default value.\n",
    "# ps.set_option(\"compute.default_index_type\", \"distributed\")  # Use default index prevent overhead.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings coming from Arrow optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:58:27.517971Z",
     "start_time": "2021-11-21T09:58:27.514383Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3957546960.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_5653/3957546960.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    uniq(df):\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "uniq(df):\n",
    "    for c in df.columns\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:58:29.324596Z",
     "start_time": "2021-11-21T09:58:29.312922Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3554035632.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_5653/3554035632.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    k=[c:df.select(c).distinct().count() for c in df.columns]\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def info(df):\n",
    "    display(df.summary().toPandas())\n",
    "    print(\"Unique values:\")\n",
    "    k=[c:df.select(c).distinct().count() for c in df.columns]\n",
    "        \n",
    "#     print(\"Nan values:\")\n",
    "#     df.select([f.count(f.when(f.isnan(c), c)).alias(c) for c in df.columns]).toPandas()\n",
    "#     print(\"Null values:\")\n",
    "#     df.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in \n",
    "#            df.columns]).toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:58:33.918507Z",
     "start_time": "2021-11-21T09:58:33.805274Z"
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/babkamen/git/udacity-big-data-engineer-nanodegree/capstone-project/data/yelp_academic_dataset_business.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/git/udacity-big-data-engineer-nanodegree/capstone-project/venv/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/udacity-big-data-engineer-nanodegree/capstone-project/venv/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/udacity-big-data-engineer-nanodegree/capstone-project/venv/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/babkamen/git/udacity-big-data-engineer-nanodegree/capstone-project/data/yelp_academic_dataset_business.json"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df=spark.read.json(\"./data/yelp_academic_dataset_business.json\")\n",
    "df.cache()\n",
    "info(df)\n",
    "# kdf=df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.029493Z",
     "start_time": "2021-11-21T09:57:28.029479Z"
    }
   },
   "outputs": [],
   "source": [
    "kdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.031076Z",
     "start_time": "2021-11-21T09:57:28.031062Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Missing values\")\n",
    "c=kdf.isnull().sum()\n",
    "print(c[c>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.031925Z",
     "start_time": "2021-11-21T09:57:28.031913Z"
    }
   },
   "outputs": [],
   "source": [
    "df.select([col[0] for col in df.dtypes if col[1] != 'string']).describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.032862Z",
     "start_time": "2021-11-21T09:57:28.032848Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kdf.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.033783Z",
     "start_time": "2021-11-21T09:57:28.033768Z"
    }
   },
   "outputs": [],
   "source": [
    "df['city'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.034645Z",
     "start_time": "2021-11-21T09:57:28.034631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=spark.read.json(\"./data/yelp_academic_dataset_review.json\")\n",
    "df.createOrReplaceTempView(\"reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.035426Z",
     "start_time": "2021-11-21T09:57:28.035412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/44413132/count-the-number-of-missing-values-in-a-dataframe-spark\n",
    "from pyspark.sql.functions import col,sum\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count empty values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.036275Z",
     "start_time": "2021-11-21T09:57:28.036259Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/data-prep-with-spark-dataframes-3629478a1041\n",
    "df.select([f.count(f.when(f.isnan(c), c)).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.037203Z",
     "start_time": "2021-11-21T09:57:28.037190Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "df.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in \n",
    "           df.columns]).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.038395Z",
     "start_time": "2021-11-21T09:57:28.038380Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter(f.isnan(\"name\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.039196Z",
     "start_time": "2021-11-21T09:57:28.039183Z"
    }
   },
   "outputs": [],
   "source": [
    "\"{:,}\".format(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.040082Z",
     "start_time": "2021-11-21T09:57:28.040069Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM reviews WHERE text='' LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.041026Z",
     "start_time": "2021-11-21T09:57:28.041013Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.041620Z",
     "start_time": "2021-11-21T09:57:28.041607Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df=spark.read.json(\"./data/yelp_academic_dataset_user.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.042262Z",
     "start_time": "2021-11-21T09:57:28.042247Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/3154460/python-human-readable-large-numbers\n",
    "\n",
    "\"{:,}\".format(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.042964Z",
     "start_time": "2021-11-21T09:57:28.042940Z"
    }
   },
   "outputs": [],
   "source": [
    "df.summary().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.045018Z",
     "start_time": "2021-11-21T09:57:28.044996Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T18:40:49.586700Z",
     "start_time": "2021-10-23T18:40:49.577881Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.046059Z",
     "start_time": "2021-11-21T09:57:28.046042Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T09:57:28.047351Z",
     "start_time": "2021-11-21T09:57:28.047330Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
